{
  "hash": "c90515139175a4668f17e3ed124d6746",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Migrating to Nextflow on AWS\"\nauthor: \"jk\"\ndescription: a short complementary guide with videos\ndate: \"2023-08-29\"\nformat: \n  html: default\ntoc: true\ncategories: [nextflow, AWS]\n---\n\n\n### Introduction\n\nI'm migrating my `Nextflow` execution to AWS from HPC. AWS setup is the main challenge here. There are several great guides that I've referenced [here](https://t-neumann.github.io/pipelines/AWS-pipeline/) and [here](https://staphb.org/resources/2020-04-29-nextflow_batch.html). The problem (if there is one) is that there is no apparent way to programmatically set up AWS for Nextflow, and the web UI changes over time, which renders those guides slightly harder to follow. The irony is that this very guide I'm writing will also end up outdated, but maybe someone will find it useful in the meantime.\n\nThe referenced guides already \n\n### Main idea\n\n![](overview.png) Stolen from [seqera labs](https://seqera.io/blog/nextflow-and-aws-batch-inside-the-integration-part-1-of-3/)\n\nA couple of tweaks:  \n-  I'm using a local Nextflow script saved in an EC2 instance running the pipeline instead of pulling it from a Github repo  \n-  I'm picking EC2 instances that will perform just one task per instance  \n\n### Creating an IAM\n\n-   create a user group with the necessary policies  \n-   create a user and add it to the user group\n-   create and download the access key  \n-   this user's credentials are needed by the node Nextflow is run from  \n\n{{< video creating-IAM.mp4>}}\n\n### Creating a role\n\n-  this role needs to be assigned to any SPOT compute environment (CE) \n\n{{< video creating-role.mp4>}}\n\n### Creating an EC2 instance to create an AMI\n\n- An AMI is an image from which EC2 instances (within an ECS cluster as defined by a CE) are created. A custom one with AWS CLI pre-installed needs to be created. Its ID needs to be referenced later.  \n\n\n{{< video creating-ec2-to-make-ami.mp4>}}\n\n::: callout-important\n-   should create a `t2.medium` instance instead of `t2.micro` or else you'd run out of memory while installing\n:::\n\n### AMI setup\n\n- `ssh` into the EC2 instance with `.pem` file saved earlier  \n  - `chmod 400` needs to be run to set the right kind of permissions  \n- I *think* `docker` automatically extends its base size to include the additional 500GB EBS volume on top of the boot volume  \n- This EC2 instance and any associated EBS volumes should be promptly deleted after its AMI is saved\n\n{{< video ami-setup.mp4>}}\n\n### AWS batch setup\n\n-   I don't touch job definition because this is set in the nextflow script  \n-   Pick instance types that match the computational demands of each process  \n    -   t-neumann's guide suggests each CE should have one type of instance to be homogeneous  \n    -   personally I grouped similar families of instances (ex. C4, C5, C5n) together to let AWS pick the optimal one  \n    -   I have one CE each for compute, memory and general purpose workloads\n-   I'm using matching names for CE and job queue \n    - feels a bit redundant, but the idea is that many job queues of different priorities can be assigned to each CE  \n\n{{< video compute-env-setup.mp4>}}\n\n### Add permission to ec2InstanceRole\n\n{{< video ec2instancerole.mp4>}}\n\n\n### Process directives\n\n`cpus`, `memory` and `time` directives will define the job definition on AWS batch. Need to account for ECS overhead in these directives to ensure that tasks will actually run (more on that in \"Monitoring\" section below). Two additional directives relevant for execution on AWS are `queue` and `container`. `queue` should match the name of the job queue we created (in turn associated with a compute environment). `container` should point to an appropriate docker container hosted online. I used [biocontainers](https://biocontainers.pro/) hosted on [quay.io](https://quay.io).\n\nI think `profiles` in the config file can be used to really separate the execution from workflow logic, but I don't see myself going back to HPC or other cloud platforms. \n\nI added `pattern` to `publishDir` directive to avoid duplicating files from `work` that I don't need to save. One case in point are processed `fastq.gz` output files form `fastp` that are consumed by `STAR`. I only copy logs as such:\n\n\n::: {.cell}\n\n```{.groovy .cell-code}\nprocess FASTP {\n  publishDir s3://{my_bucket}/out/fastp, mode: \"copy\", pattern: \"*{html,json}\"\n}\n\n```\n:::\n\n\n### Create an EC2 instance for running Nextflow through AWS Batch\n\nI use a `t2.medium` instance and install AWS CLI as before (creating an AMI). In this instance (not literally, referring to EC2), I'm running `aws configure` using the credentials copied/saved earlier (creating an IAM). Official instructions and security recommendations [here](https://docs.aws.amazon.com/cli/latest/userguide/getting-started-quickstart.html). My Nextflow script is saved in the home directory of this instance.  \n\n### Nextflow config file\n\nThis is my `nextflow.config` file.  \n\n\n::: {.cell}\n\n```{.groovy .cell-code  code-line-numbers=\"true\"}\nplugins {\n    id 'nf-amazon'\n}\n\n\naws {\n    region = 'us-east-1'\n    batch {\n        cliPath = '/home/ec2-user/miniconda/bin/aws'\n    }\n}\n\nprocess {\n    executor = 'awsbatch'\n}\n\n```\n:::\n\n\n### Running a pipeline\n\nNextflow job submission to AWS Batch is **not** interrupted even if my `ssh` connection to the EC2 instance is lost. \n\nI think the `work` and `out` sub directories should already exist in the `s3` bucket.\n\n::: {.cell}\n\n```{.bash .cell-code}\nnextflow run main.nf -w s3://{your_bucket}/work --outputDir s3://{your_bucket}/out -resume\n```\n:::\n\n\n\n### Monitoring progress (or lack thereof)\n\nFrom the AWS Batch dashboard under 'job queue overview', tasks should go from submitted, runnable, starting, running to succeeded. The crux is going from runnable to running. If something is not set up properly (CE, job queue, Nextflow directives), runnable jobs will never actually run. For me, only a couple of jobs would run simultaneously even when my CE was set to use beefy EC2 instances on-demand. This was due to the default EC2 quota of a few dozen vCPUs. From the root AWS account, [Service Quota](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-resource-limits.html) can be accessed and a request to increase this # can be put in (I requested 500 vCPUs and got 300 approved). \n\n::: callout-warning\n\nTo quote the late Uncle Ben, with great [vCPU quota], comes great responsibility (think cost management and security).  \n\n:::\n\n### Aftermath\n\nOnce the smoke clears and dust settles, I'm left with precious output files in my `s3` bucket ready to be downloaded. I use credentials `configure`'d AWS CLI on my laptop.\n\n\n::: {.cell}\n\n```{.bash .cell-code .code-overflow-wrap}\naws s3 cp s3://my_bucket/out . --recursive\n\n```\n:::\n\n\n::: center\n\\\\ (•◡•) /\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}